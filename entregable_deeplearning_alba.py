# -*- coding: utf-8 -*-
"""Entregable_DeepLearning_Alba (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14-yyO-ZLewUbryEVSj-UFFJTd5Pt25RN

# **Proyecto final Deep Learning Alba ZÚÑIGA ASPAS**

Comenzamos instalando tres librerías, las vamos a utilizar durante la práctica. TensorFlow, Keras (es parte de TensorFlow) y kaggle para la descarga del conjunto de datos.
"""

!pip install tensorflow keras kaggle

from google.colab import files
files.upload() # donde cargo el archivo kaggle.json para la API y poder descargar el conjunto de datos

# creamos el directorio oculto donde copiamos el json y se otorgan permisos de lectura y escritura solo para mi usuario en este caso
# esto asegura que la biblioteca de Kaggle pueda autenticarse correctamente usando la API Key para descargar datasets
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d stevezhenghp/airbnb-price-prediction
!unzip airbnb-price-prediction.zip

import pandas as pd

# Cargamos el archivo CSV
df = pd.read_csv('train.csv')

# Mostramos las primeras filas del df
print(df.head())

# información general del DataFrame
print(df.info())

# estadísticas descriptivas de las columnas numéricas
print(df.describe())

# verificamos si hay valores nulos
print(df.isnull().sum())

#esta celda no tiene sentido ya que hemos podido ver que log_price no tiene nuelos. Pero eliminaríamos cualquier fila que tenga un valor nulo en la columna 'log_price', que es la columna objetivo
df = df.dropna(subset=['log_price'])

"""El enfoque va a ser naif. Vamos a ver que aunque he incluido la función de download_images yo he preferido hacer la opción de GDrive"""

import cv2
import numpy as np
import imageio.v3 as io
from tqdm import tqdm
from typing import Optional, Union

def download_images(paths: list,
                    canvas: tuple = (224, 224),
                    nb_channels: int = 3,
                    max_imgs: Optional[int] = None) -> tuple:
    """
    Download a list of images from url addresses, converting them to a specific canvas size.

    Args:
        paths: Paths or url addresses from which to load images.
        canvas: Desired image width and height.
        nb_channels: Channels in images (1 for B/W, 3 for RGB).
        max_imgs: Upper threshold in the number of images to download.

    Return:
        a tuple of:
        - image values
        - indices within the paths that were successful.
    """
    n_images = len(paths) if not max_imgs else max_imgs
    images = np.zeros((n_images, canvas[0], canvas[1], nb_channels), dtype=np.uint8)
    downloaded_idxs = []

    for i_img, url in enumerate(tqdm(paths, total=n_images)):
        if i_img >= n_images:
            break
        try:
            img = io.imread(url)
            img = cv2.resize(img, (canvas[0], canvas[1]))
            downloaded_idxs.append(i_img)
            images[i_img] = img
        except (IOError, ValueError) as e:
            pass  # Unavailable url / conversion error

    return images[downloaded_idxs], downloaded_idxs

"""**Carga de Datos e Imágenes desde Google Drive**"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np

# cargamos el archivo CSV
data = pd.read_csv('/content/drive/MyDrive/keepcoding_DL/data_KC.csv', sep=';')

# cargamos las imágenes desde un archivo numpy
images = np.load('/content/drive/MyDrive/keepcoding_DL/images_KC.npy')

# mostramos las formas de los datos y las imágenes para verificación
print(data.shape, images.shape)

"""Realizamos la conversión de log_price a Price utilizando numpy.exp. Una práctica común en análisis de datos y machine learning para manejar mejor la variabilidad en los datos y facilitar la interpretación de los resultados."""

import numpy as np

# convertimos los valores logarítmicos de precios a precios originales
data["Price"] = np.exp(data["log_price"])

# mostramos las primeras filas del DataFrame para verificar la conversión
print(data.head())

"""Guardamos las imágenes y los datos procesados en Google Drive para futuros usos."""

# guardamos en GDrive
np.save('images_KC.npy', images)
data.to_csv('data_KC.csv', sep=';', index=False)

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

"""Verificamos:"""

!cp images_KC.npy /content/drive/MyDrive/keepcoding_DL/images_KC.npy
!cp data_KC.csv /content/drive/MyDrive/keepcoding_DL/data_KC.csv

!ls -lah images* data*  # comprobación

"""**Preparamos las etiquetas de clasificación**

Estadísticas como el conteo, la media, la desviación estándar, los valores mínimo y máximo, y los percentiles del 25%, 50% y 75%.
"""

data.describe()

"""Importaciones de bibliotecas necesarias para análisis estadístico, preprocesamiento de datos, división de datos en conjuntos de entrenamiento y prueba, y construcción de modelos de deep learning."""

from scipy.stats import spearmanr
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras import Model
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten, GlobalAveragePooling2D
from tensorflow.keras.applications import VGG19, ResNet50

"""Clasificamos los precios en tres categorías diferentes para facilitar un análisis de clasificación en lugar de regresión."""

y_class = []
for y in data['Price']:
  if y <= 80:
    y_class.append(0)
  elif 80 < y <= 140:
    y_class.append(1)
  else:
    y_class.append(2)
y_class = pd.Series(y_class)
y_class.hist(bins=3)

""" Aseguramos que las etiquetas de clasificación (y_class) tengan el mismo índice que el DataFrame data."""

# reindexamos
y_class.index = data.index

"""Asignamos la columna Price a la variable ground_truth. Y mostramos histograma."""

ground_truth = data['Price']
ground_truth.hist(bins=50)

"""**División en train validation test**"""

from sklearn.model_selection import train_test_split

# división inicial en entrenamiento y prueba
train_df, test_df, train_y, test_y = train_test_split(data, y_class,
                                                      test_size=0.25,
                                                      random_state=42,
                                                      shuffle=True,
                                                      stratify=y_class)

# división secundaria en entrenamiento y validación
train_df, val_df, train_y, val_y = train_test_split(train_df, train_y,
                                                    test_size=0.15,
                                                    random_state=42,
                                                    shuffle=True,
                                                    stratify=train_y)

# imprimimos formas de los conjuntos
print(train_df.shape, val_df.shape, test_df.shape)
print(train_y.shape, val_y.shape, test_y.shape)

# almacenamos los índices correspondientes a las particiones
train_idx, val_idx, test_idx = train_df.index, val_df.index, test_df.index

# verificamos la consistencia de las particiones
print(data.loc[train_idx].shape)
print(y_class.loc[train_idx].shape)

"""**Procesado de los datos**

*   Datos tabulares - numéricos
*   Datos tabulares - categóricos
"""

numerical_cols = ['accommodates', 'bathrooms', 'bedrooms', 'beds', 'latitude', 'longitude', 'number_of_reviews', 'review_scores_rating']
data[numerical_cols].shape

for col in numerical_cols:
    data[col] = data[col].fillna(0)

data[numerical_cols].head(5)

# normalización de los datos numéricos
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
train_num = pd.DataFrame(scaler.fit_transform(train_df[numerical_cols]), columns=numerical_cols)
val_num = pd.DataFrame(scaler.transform(val_df[numerical_cols]), columns=numerical_cols)
test_num = pd.DataFrame(scaler.transform(test_df[numerical_cols]), columns=numerical_cols)

numerical_data = pd.concat([train_num, val_num, test_num], axis=0)
numerical_data.index = data.index
numerical_data.shape

"""Vamos con los categóricos"""

categorical_cols = ['property_type', 'room_type', 'cancellation_policy']
data[categorical_cols].head(5)

# conversión a variables dummy (one-hot encoding)

property_type = pd.get_dummies(data[['property_type']], prefix='property_type')
room_type = pd.get_dummies(data[['room_type']], prefix='room_type')
canc_pol = pd.get_dummies(data['cancellation_policy'], prefix='cancellation_policy')

categorical_data = pd.concat([property_type, room_type, canc_pol], axis=1, join='inner')
categorical_data.shape

"""**Juntamos ambos tipos de datos**

Concatenamos las columnas numéricas normalizadas y las columnas categóricas codificadas en un solo DataFrame. Generamos pues un conjunto de datos final que combina todas las características que se utilizarán para entrenar el modelo.
"""

data = pd.concat([numerical_data, categorical_data], axis=1)
data

"""**Procesado de valores NaN**"""

data.isna().sum()

"""Rellenamos los valores nulos en las columnas seleccionadas con la media de las otras muestras de esa columna. Así tratamos los datos faltantes y evitamos que afecten negativamente al modelo."""

cols2fill = ['bathrooms', 'bedrooms', 'beds', 'review_scores_rating']
for col in cols2fill:
    data[col] = data[col].fillna(data[col].mean())
data.isna().sum()

"""**Clasificación basada en datos tabulares (FC layers)**

**Escalado [0, 1]**

**Normalización de datos entre 0 y 1**
"""

# vemos rangos máximos
data.max(axis=0)

# escalamos todas las características en el rango [0, 1].
mm_scaler = MinMaxScaler()
cols = data.columns

train_X = pd.DataFrame(mm_scaler.fit_transform(data.loc[train_idx]), columns=cols)
val_X = pd.DataFrame(mm_scaler.transform(data.loc[val_idx]), columns=cols)
test_X = pd.DataFrame(mm_scaler.transform(data.loc[test_idx]), columns=cols)

# verificamos
train_X.max(axis=0)

"""**Clasificación con datos tabulares**

**Conversión de etiquetas a one-hot**
"""

# convertimos las etiquetas de clase en formato one-hot encoding necesario para que la función de pérdida categorical_crossentropy pueda ser utilizada en la clasificación
train_y_cls = to_categorical(y_class.loc[train_idx])
val_y_cls = to_categorical(y_class.loc[val_idx])
test_y_cls = to_categorical(y_class.loc[test_idx])

"""**Creación y entrenamiento del modelo**"""

# definimos el modelo:
nb_out = train_y_cls.shape[1]
classifier1D = Sequential()
classifier1D.add(Dense(64, input_shape=(data.shape[1],), activation='relu'))
classifier1D.add(Dense(32, activation='relu'))
classifier1D.add(Dense(8, activation='relu'))
classifier1D.add(Dense(nb_out, activation='softmax'))
# compilamos el modelo:
classifier1D.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
# resumen del modelo:
classifier1D.summary()

"""**Entrenamiento del modelo**

El modelo se entrena durante 30 épocas con un tamaño de lote de 32. Los datos de validación se utilizan para monitorear la precisión y la pérdida durante el entrenamiento.
"""

EPOCHS = 30
BS = 32

H = classifier1D.fit(x=train_X, y=train_y_cls, batch_size=BS, epochs=EPOCHS,
          validation_data=(val_X, val_y_cls), shuffle=True, verbose=1)

"""Graficamos las curvas de pérdida y precisión tanto para los datos de entrenamiento como de validación a lo largo de las épocas. Esto nos ayuda a visualizar cómo está aprendiendo el modelo y si hay signos de sobreajuste."""

import matplotlib.pyplot as plt
plt.plot(H.history['loss'], label='train_loss')
plt.plot(H.history['accuracy'], label='train_acc')
plt.plot(H.history['val_loss'], label='val_loss')
plt.plot(H.history['val_accuracy'], label='val_acc')
plt.legend()
plt.xlabel('EPOCH')
plt.plot()

"""Evaluamos el modelo en el conjunto de datos de prueba para obtener la precisión y la pérdida final."""

loss, acc = classifier1D.evaluate(test_X, test_y_cls)
print(f'Loss={loss}, Acc={acc}')

"""**Regresión con datos tabulares (FC layers)**

**Escalado de las etiquetas**
"""

# utilizamos MinMaxScaler para escalar los precios (ground_truth) entre 0 y 1 para todas las particiones de datos (entrenamiento, validación y prueba).
mm_scaler = MinMaxScaler()

train_y_reg = mm_scaler.fit_transform(
    ground_truth.loc[train_idx].values.reshape(-1, 1))
val_y_reg = mm_scaler.transform(
    ground_truth.loc[val_idx].values.reshape(-1, 1))
test_y_reg = mm_scaler.transform(
    ground_truth.loc[test_idx].values.reshape(-1, 1))

train_y_reg.shape, val_y_reg.shape, test_y_reg.shape

"""**Creación y entrenamiento del modelo de regresión**

Creamos un modelo Sequential de Keras.
Añadimos varias capas densas (Dense) con funciones de activación ReLU para capturar relaciones no lineales.
La capa final utiliza una función de activación lineal para la regresión.
"""

# definimos el modelo de regresión:
nb_out = train_y_reg.shape[1]
regressor1D = Sequential()
regressor1D.add(Dense(64, input_shape=(data.shape[1],), activation='relu'))
regressor1D.add(Dense(32, activation='relu'))
regressor1D.add(Dense(8, activation='relu'))
regressor1D.add(Dense(nb_out, activation='linear'))

# compilamos
regressor1D.compile(loss='mean_squared_error', optimizer='adam')
regressor1D.summary()

"""**Entrenamiento del modelo**

30 épocas con un tamaño de lote de 32. Los datos de validación se utilizan para monitorear la pérdida durante el entrenamiento.
"""

EPOCHS = 30
BS = 32

H = regressor1D.fit(x=train_X, y=train_y_reg, batch_size=BS, epochs=EPOCHS,
          validation_data=(val_X, val_y_reg), shuffle=True, verbose=1)

"""**Evaluación del modelo y visualización de resultados**"""

plt.plot(H.history['loss'], label='train_loss')
plt.plot(H.history['val_loss'], label='val_loss')
plt.legend()
plt.xlabel('EPOCH')
plt.plot()

"""**Evaluación y análisis del modelo**

Evaluamos el modelo en el conjunto de datos de prueba para obtener la pérdida final. También calculamos el coeficiente de correlación de Spearman para evaluar la relación entre los precios reales y predichos.
"""

# evaluamos el modelo en el conjunto de prueba:
loss = regressor1D.evaluate(test_X, test_y_reg)
rho = spearmanr(test_y_reg, regressor1D.predict(test_X))[0]
print(f'Loss (MSE)={loss:.3f}, Spearman Rank Correlation Coefficient={rho:.3f}')
# representación de precios reales y predichos:
test_prices = ground_truth.loc[test_idx]
raw_preds = regressor1D.predict(test_X)
rescaled_preds = mm_scaler.inverse_transform(raw_preds)

plt.hist(test_prices, bins=50, label="precio_real", alpha=0.8)
plt.hist(rescaled_preds, bins=50, label="precio_predicho", alpha=0.8)
plt.legend()
plt.show()

"""# **Definición del input shape y carga del modelo pre-entrenado:**"""

input_shape = images[0].shape
base_resnet = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)
base_resnet.summary()

"""**Transfer-learning**

**Congelar capas del modelo base y agregar capas de clasificación:**
"""

for layer in base_resnet.layers:
    layer.trainable = False

DROPOUT = 0.5
nb_out = train_y_cls.shape[1]
last = base_resnet.layers[-1].output
x = GlobalAveragePooling2D()(last)
x = Dense(256, activation='relu', name='fc1')(x)
x = Dropout(DROPOUT)(x)
x = Dense(nb_out, activation='softmax', name='predictions')(x)
classifier2D = Model(base_resnet.input, x)

classifier2D.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
classifier2D.summary()

"""**Entrenamiento del modelo:**

Hiperparámetros de entrenamiento: definimos el número de épocas (EPOCHS) y el tamaño del lote (BS).
Entrenamiento: entrenamos el modelo con los datos de entrenamiento y validamos con los datos de validación.
"""

EPOCHS = 5
BS = 32

H = classifier2D.fit(images[train_idx], train_y_cls, batch_size=BS, epochs=EPOCHS,
          validation_data=(images[val_idx], val_y_cls), shuffle=True, verbose=1)

"""**Evaluación del modelo y visualización de las curvas de aprendizaje:**"""

plt.plot(H.history['loss'], label='train_loss')
plt.plot(H.history['accuracy'], label='train_acc')
plt.plot(H.history['val_loss'], label='val_loss')
plt.plot(H.history['val_accuracy'], label='val_acc')
plt.legend()
plt.xlabel('EPOCH')
plt.plot()

"""**Evaluación final del modelo en el conjunto de test:**"""

loss, acc = classifier2D.evaluate(images[test_idx], test_y_cls)
print(f'Loss={loss}, Acc={acc}')

"""**Regresión basada en imágenes (CNN)**

**Configuración del modelo de regresión:**

Similar al modelo de clasificación, pero la capa de salida usa una activación lineal y la pérdida es el error cuadrático medio.
"""

DROPOUT = 0.5
nb_out = train_y_reg.shape[1]
last = base_resnet.layers[-1].output
x = GlobalAveragePooling2D()(last)
x = Dense(256, activation='relu', name='fc1')(x)
x = Dropout(DROPOUT)(x)
x = Dense(nb_out, activation='linear', name='predictions')(x)
regressor2D = Model(base_resnet.input, x)

regressor2D.compile(optimizer='adam', loss='mean_squared_error')
regressor2D.summary()

"""**Entrenamiento del modelo de regresión:**

Similar al entrenamiento del modelo de clasificación, pero con las etiquetas de regresión.
"""

EPOCHS = 5
BS = 32

H = regressor2D.fit(x=images[train_idx], y=train_y_reg, batch_size=BS, epochs=EPOCHS,
          validation_data=(images[val_idx], val_y_reg), shuffle=True, verbose=1)

# graficamos las pérdidas de entrenamiento y validación para la regresión.
plt.plot(H.history['loss'], label='train_loss')
plt.plot(H.history['val_loss'], label='val_loss')
plt.legend()
plt.xlabel('EPOCH')
plt.plot()

"""Evaluamos el modelo con el conjunto de test y calculamos el coeficiente de correlación de Spearman para medir la relación entre los valores predichos y los reales."""

loss = regressor2D.evaluate(images[test_idx], test_y_reg)
rho = spearmanr(test_y_reg, regressor2D.predict(images[test_idx]))[0]
print(f'Loss (MSE)={loss:.3f}\nSpearman Rank Correlation Coefficient={rho:.3f}')
# graficamos los precios reales y los predichos para visualizar la precisión del modelo:
test_prices = ground_truth.loc[test_idx]
raw_preds = regressor2D.predict(images[test_idx])
rescaled_preds = mm_scaler.inverse_transform(raw_preds)

plt.hist(test_prices, bins=50, label="precio_real", alpha=0.8)
plt.hist(rescaled_preds, bins=50, label="precio_predicho", alpha=0.8)
plt.legend()
plt.show()

"""# **Combinación de fuentes**

Vamos a considerar la combinación de modelos de clasificación, con dos enfoques principales: Late-Fusion y Early-Fusion.

**A) Late-Fusion (predicciones 1D + predicciones 2D)**

**Calculamos predicciones train/val/test de classifier1D**
"""

late_1d_train = classifier1D.predict(train_X)
late_1d_val = classifier1D.predict(val_X)
late_1d_test = classifier1D.predict(test_X)
print(late_1d_train.shape)

# análogo para classifier2D
late_2d_train = classifier1D.predict(train_X)
late_2d_val = classifier1D.predict(val_X)
late_2d_test = classifier1D.predict(test_X)
print(late_2d_train.shape)

"""**Combinamos las predicciones y entrenar un SVM**"""

from sklearn.svm import SVC
late_train_feats = np.concatenate((late_1d_train, late_2d_train), axis=1)
late_val_feats = np.concatenate((late_1d_val, late_2d_val), axis=1)
late_test_feats = np.concatenate((late_1d_test, late_2d_test), axis=1)

fusion_labels_train = np.concatenate((y_class.loc[train_idx], y_class.loc[val_idx]))
fusion_labels_test = y_class.loc[test_idx]

late_fuser = SVC(random_state=42)
late_fuser.fit(X=np.concatenate((late_train_feats, late_val_feats)), y=fusion_labels_train)
acc_late = late_fuser.score(X=late_test_feats, y=fusion_labels_test)
print(f"Late fusion achieves {acc_late:0.3f} accuracy")

"""**B) Early-Fusion (representación 1D + representación 2D)**

**Concatenar representaciones 1D**

Se concatenan las representaciones 1D de los conjuntos de entrenamiento y validación.
"""

early_1d_train = np.concatenate((train_X, val_X), axis=0)
print(early_1d_train.shape)

"""**Definimos un nuevo modelo para obtener características de 2D**

Tomamos una capa intermedia de classifier2D como extractor de características.
"""

last_conv = classifier2D.layers[-4].output
feature_extractor = Model(classifier2D.input, last_conv)
feature_extractor.summary()

"""**Obtenemos características de imágenes**

Obtenemos las características de las imágenes usando el extractor definido.
"""

early_2d_train = feature_extractor.predict(images[train_idx])
early_2d_val = feature_extractor.predict(images[val_idx])
early_2d_test = feature_extractor.predict(images[test_idx])
print(early_2d_train.shape)

"""**Reducimos dimensionalidad con PCA**

Dado que las características obtenidas de las imágenes pueden tener alta dimensionalidad, se reduce esta usando PCA.
"""

from sklearn.decomposition import PCA

pca = PCA(n_components=50, random_state=42)
early_2d_train_pca = pca.fit_transform(early_2d_train)
early_2d_val_pca = pca.transform(early_2d_val)
early_2d_test_pca = pca.transform(early_2d_test)
print(early_2d_train_pca.shape)

"""**Combinamos ambas representaciones y entrenamos SVM**

Se combinan las representaciones 1D y las características PCA de las imágenes para entrenar y evaluar un modelo SVM.
"""

# Combinamos ambas representaciones (tabular + features-PCA)
early_train_feats = np.concatenate((early_1d_train, np.concatenate((early_2d_train_pca, early_2d_val_pca))), axis=1)
early_test_feats = np.concatenate((test_X, early_2d_test_pca), axis=1)

early_fuser = SVC(random_state=42)
early_fuser.fit(X=early_train_feats, y=fusion_labels_train)
acc_early = early_fuser.score(X=early_test_feats, y=fusion_labels_test)
print(f"Late fusion achieves {acc_early:0.3f} accuracy")

"""# **Cálculo de bandas de error**

Finalmente, calculamos las bandas de error para entender el margen de error de cada modelo:
"""

ci = lambda score, N: 1.96 * np.sqrt((score * (1 - score)) / N)
low_late_fusion = acc_late - ci(acc_late, len(test_X))
upper_late_fusion = acc_late + ci(acc_late, len(test_X))
low_early_fusion = acc_early - ci(acc_early, len(test_X))
upper_early_fusion = acc_early + ci(acc_early, len(test_X))

print(f"[LATE-fusion] accuracy con 95% error: [{low_late_fusion: .3f}, {upper_late_fusion: .3f}]")
print(f"[EARLY-fusion] accuracy con 95% error: [{low_early_fusion: .3f}, {upper_early_fusion: .3f}]")

"""Esto da una idea del intervalo de confianza del 95% para la precisión de los modelos."""

